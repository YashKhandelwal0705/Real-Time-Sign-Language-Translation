# Sign Language Translation System

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A real-time Sign Language Translation system that converts hand gestures into text using computer vision and deep learning.

## ğŸš€ Features

- ğŸ“± Real-time hand gesture detection using MediaPipe
- ğŸ¤ Support for 4 gestures: A, F, L, Y
- ğŸŒ Web-based interface with Flask
- ğŸ“º Real-time video streaming
- ğŸ›¡ï¸ Robust error handling and frame boundary detection
- ğŸ“± Mobile-friendly interface

## ğŸ› ï¸ Tech Stack

- **Backend**: Python 3.12+
- **Computer Vision**: OpenCV, MediaPipe
- **Deep Learning**: YOLOv8
- **Web Framework**: Flask
- **Model**: PyTorch

## ğŸ“¥ Installation

1. Clone the repository:
```bash
git clone https://github.com/YashKhandelwal0705/Sign-Language-Translation-System.git
cd Sign-Language-Translation-System
```

2. Create and activate virtual environment:
```bash
python -m venv venv
.\venv\Scripts\activate  # Windows
# or
source venv/bin/activate   # Unix/Linux
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸš€ Usage

1. Run the Flask application:
```bash
python "Flask App/app.py"
```

2. Open your web browser and navigate to:
```
http://127.0.0.1:5000/
```

3. Point your webcam at your hand and make gestures (A, F, L, Y)

## ğŸ“ Project Structure

```
Sign-Language-Translation-System/
â”œâ”€â”€ Flask App/             # Web application
â”‚   â”œâ”€â”€ app.py            # Main Flask application
â”‚   â””â”€â”€ templates/        # HTML templates
â”œâ”€â”€ Python Files/         # Core Python scripts
â”‚   â”œâ”€â”€ Yolo.py          # YOLO model implementation
â”‚   â”œâ”€â”€ Feature Extraction.py  # Data preprocessing
â”‚   â”œâ”€â”€ train.py         # Model training script
â”‚   â””â”€â”€ ...              # Other utility scripts
â”œâ”€â”€ Dataset/             # Training dataset
â”œâ”€â”€ runs/               # Model training outputs
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ README.md          # Project documentation
â””â”€â”€ LICENSE            # License file
```

## ğŸ¯ How it Works

1. The system uses MediaPipe's hand tracking module to detect hands in real-time
2. Hand regions are extracted and normalized
3. The YOLOv8 model classifies the gestures into one of the supported categories
4. Results are displayed in real-time via a Flask web interface

## ğŸ“± Supported Gestures
- A: Closed fist with thumb up
- F: Five fingers spread
- L: L-shaped hand
- Y: Y-shaped hand (V sign)

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details

## ğŸ‘¥ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ¤ Acknowledgments

- Thanks to the MediaPipe team for their excellent hand tracking module
- Thanks to the YOLOv8 team for their efficient object detection framework
- Thanks to Flask for providing a simple web framework

## ğŸ“ Contact

Yash Khandelwal - [GitHub](https://github.com/YashKhandelwal0705)

Project Link: [https://github.com/YashKhandelwal0705/Sign-Language-Translation-System](https://github.com/YashKhandelwal0705/Sign-Language-Translation-System)
